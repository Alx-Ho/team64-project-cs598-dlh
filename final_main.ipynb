{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3WwDj9HsJYs"
   },
   "source": [
    "https://drive.google.com/drive/folders/1_bGKFHFaYi1eEcCMs8Jv9LthpwimVA_8?usp=share_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUdyovKzHekN"
   },
   "source": [
    "https://github.com/Alx-Ho/team64-project-cs598-dlh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmU1C_vce0rQ",
    "outputId": "ab9dc011-cee1-468b-cd44-89044ca6c2c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2023.9.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYC549OZPWU_",
    "outputId": "178f44ea-4cd9-41a3-9008-7e9898724879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'team64-project-cs598-dlh' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Alx-Ho/team64-project-cs598-dlh.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "RavspJ2fT25T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import pandas as pd\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQsp2C4renaM"
   },
   "source": [
    "**Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARI7NNSx7A5p"
   },
   "source": [
    "Predicting drug-drug interactions (DDIs) is challenging due to the complex nature of biological systems and the vast number of possible drug and food combinations. The human body's response to these interactions can vary significantly, making accurate predictions difficult. A major hurdle is the lack of comprehensive data on many drugs and food constituents, including their effects on the body and their metabolic pathways. Moreover, the mechanisms through which DDIs occur are diverse, adding another layer of complexity to prediction efforts. The constant development of new drugs and the updating of existing drug information necessitate continuous revisions of prediction models and databases.\n",
    "\n",
    "The paper presents a computational framework named DeepDDI [1], designed to predict drugâ€“drug interactions (DDIs) using only the structural information of the drug constituent pairs as inputs. DeepDDI employs a deep neural network (DNN) optimized to accurately predict 86 DDI types, generating predictions as human-readable sentences with a mean accuracy of 92.4% using the DrugBank gold standard DDI dataset. The input structural information is provided in the Simplified Molecular-Input Line-Entry System (SMILES) format, which describes the chemical compound's structure. This framework aims to enhance the understanding of DDIs and DFIs, providing critical information for drug prescription and dietary suggestions during medication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFvqDwJDerZR"
   },
   "source": [
    "**Scope of Reproduciblity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKBTT0pl7DzM"
   },
   "source": [
    "The proposed model works on SMILES text representations for molecules, and is thus very memory efficient and accessible computational-wise for reproducibility. However, the original paper's repository https://bitbucket.org/kaistsystemsbiology/deepddi/src/master/ did not contain the training scripts for their deep learning algorithm nor the model definition. The ChemicalX library https://github.com/AstraZeneca/chemicalx contains an implemenation of the model as described in the original paper, but it is not maintained so it proved difficult to get the environment set up so that it would work in Google Colab's environment. Therefore, we implemented the model without the use of the ChemicalX library, and instead defined a new PyTorch model class using the ChemicalX implemenation as a reference.\n",
    "\n",
    "Training to the full extent of the original paper is difficult, however. The full dataset contains 192,303 DDI samples and was trained for up to 100 epochs. Due to compute credit limits with Google Colab, we limited training to 5 epochs and the dataset to 8,192 samples for training, 2,048 for validation, and 2,048 for hold-out testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEcFK3b_exq7"
   },
   "source": [
    "**Methodology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BU_j2Dvalwqi",
    "outputId": "f5f7c522-334a-43c7-ea52-36a42ba11f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhYRLqt_lysD",
    "outputId": "29c8f56e-d3cb-4829-aefa-ceb8b6970f8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.2.1+cu121\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
      "Required-by: fastai, torchaudio, torchdata, torchtext, torchvision\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FN8Ymwpl810",
    "outputId": "915327b0-8415-4671-e82e-63f40fa2d04d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pandas\n",
      "Version: 2.0.3\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
      "License: BSD 3-Clause License\n",
      "        \n",
      "        Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
      "        All rights reserved.\n",
      "        \n",
      "        Copyright (c) 2011-2023, Open source contributors.\n",
      "        \n",
      "        Redistribution and use in source and binary forms, with or without\n",
      "        modification, are permitted provided that the following conditions are met:\n",
      "        \n",
      "        * Redistributions of source code must retain the above copyright notice, this\n",
      "          list of conditions and the following disclaimer.\n",
      "        \n",
      "        * Redistributions in binary form must reproduce the above copyright notice,\n",
      "          this list of conditions and the following disclaimer in the documentation\n",
      "          and/or other materials provided with the distribution.\n",
      "        \n",
      "        * Neither the name of the copyright holder nor the names of its\n",
      "          contributors may be used to endorse or promote products derived from\n",
      "          this software without specific prior written permission.\n",
      "        \n",
      "        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "        AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "        IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "        DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "        FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "        DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "        SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "        CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "        OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "        OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "        \n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: numpy, python-dateutil, pytz, tzdata\n",
      "Required-by: altair, arviz, bigframes, bokeh, bqplot, cmdstanpy, cudf-cu12, cufflinks, datascience, db-dtypes, dopamine-rl, fastai, geemap, geopandas, google-colab, gspread-dataframe, holoviews, ibis-framework, mizani, mlxtend, pandas-datareader, pandas-gbq, panel, plotnine, prophet, pymc, seaborn, sklearn-pandas, statsmodels, vega-datasets, xarray, yfinance\n"
     ]
    }
   ],
   "source": [
    "!pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmkM0-jkl_x6",
    "outputId": "d7739e15-29c5-4925-f07a-b3f190124876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: rdkit\n",
      "Version: 2023.9.6\n",
      "Summary: A collection of chemoinformatics and machine-learning software written in C++ and Python\n",
      "Home-page: https://github.com/kuelumbus/rdkit-pypi\n",
      "Author: Christopher Kuenneth\n",
      "Author-email: chris@kuenneth.dev\n",
      "License: BSD-3-Clause\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: numpy, Pillow\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSUDvAYBmDVP",
    "outputId": "979e6e7b-c764-4bcb-8ee0-79a85cd85eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\n",
      "Version: 1.25.2\n",
      "Summary: Fundamental package for array computing in Python\n",
      "Home-page: https://www.numpy.org\n",
      "Author: Travis E. Oliphant et al.\n",
      "Author-email: \n",
      "License: BSD-3-Clause\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: \n",
      "Required-by: albumentations, altair, arviz, astropy, autograd, blis, bokeh, bqplot, chex, cmdstanpy, contourpy, cudf-cu12, cufflinks, cupy-cuda12x, cvxpy, datascience, db-dtypes, dopamine-rl, ecos, flax, folium, geemap, gensim, gym, h5py, holoviews, hyperopt, ibis-framework, imageio, imbalanced-learn, imgaug, jax, jaxlib, librosa, lightgbm, matplotlib, matplotlib-venn, missingno, mizani, ml-dtypes, mlxtend, moviepy, music21, nibabel, numba, numexpr, opencv-contrib-python, opencv-python, opencv-python-headless, opt-einsum, optax, orbax-checkpoint, osqp, pandas, pandas-gbq, pandas-stubs, patsy, plotnine, prophet, pyarrow, pycocotools, pyerfa, pymc, pytensor, python-louvain, PyWavelets, qdldl, qudida, rdkit, rmm-cu12, scikit-image, scikit-learn, scipy, scs, seaborn, shapely, sklearn-pandas, soxr, spacy, stanio, statsmodels, tables, tensorboard, tensorflow, tensorflow-datasets, tensorflow-hub, tensorflow-probability, tensorstore, thinc, tifffile, torchtext, torchvision, transformers, wordcloud, xarray, xarray-einstats, xgboost, yellowbrick, yfinance\n"
     ]
    }
   ],
   "source": [
    "!pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXCaozNxmF8M",
    "outputId": "97f839a6-9bb8-4ead-a113-1d5ddea4266b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.2.2\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: http://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: new BSD\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: bigframes, fastai, imbalanced-learn, librosa, mlxtend, qudida, sklearn-pandas, yellowbrick\n"
     ]
    }
   ],
   "source": [
    "!pip show scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKGkb3B1-gZ-"
   },
   "source": [
    "*Data*\n",
    "\n",
    "We are using the datasets listed in the Supporting Information section for the PNAS publication link: https://www.pnas.org/doi/suppl/10.1073/pnas.1803294115\n",
    "\n",
    "The two that we need for this project are Datasets S01 and S02, which are also under `data/supplement_excel` in the GitHub project. S01 describes the different Drug-Drug Interaction (DDI) types and the sentence structure associated with each. S02 contains the base data which was used in the original paper, however it is missing some crucial information necessary for model training including the SMILES representation and the DDI type. This notebook will primarily process S02 to create a ready-to-use dataset for model training.\n",
    "\n",
    "The dataset provided for this project came in the form of Drug Bank IDs for each pair of drugs that have an interaction specified as a DDI type. For example,\n",
    "\n",
    "`The metabolism of DB01621 can be decreased when combined with DB00477.,training`\n",
    "\n",
    "From the text and format, we can also determine that this is DDI type 6 based on a table provided with the original paper. It also indicates that this sample with drug pair DB01621 (drug B) and DB00477 (drug A) was used in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63upzasiiNOT"
   },
   "source": [
    "*Preprocessing (in GitHub notebook `preprocess_ds.ipynb`, conda environment provided)*\n",
    "\n",
    "First, Dataset S02 is manually converted to CSV format and columns renamed\n",
    "\n",
    "'Sentences describing the reported drug-drug interactions' --> 'gt'\n",
    "\n",
    "'Data type used to optimize the DNN architecture' --> 'set'\n",
    "\n",
    "The resulting file is in 'data/ds_s2.csv'\n",
    "\n",
    "We can see from reading through Dataset S01 that there are three types of sentence structures for Dataset S02 regarding the order of the drugs mentioned:\n",
    "\n",
    "1. Drug A ... Drug B ... (AB)\n",
    "2. Drug B ... Drug A ... (BA)\n",
    "3. Drug B ... Drug B ... Drug A (BBA)\n",
    "\n",
    "The DDI types were manually sorted into these three types and saved in `ddi_types_ab.csv`, `ddi_types_ba.csv`, and `ddi_types_bba.csv` where the column `type` is the DDI type listed in Dataset S01, and the column `structure` is the original sentence structure listed in Dataset S01. From these structures, we can create regular expression patterns to extract the Drug Bank ID from Dataset S02.\n",
    "\n",
    "Next, we extract the Drug Bank IDs for Drug A and Drug B from the ground truth sentences in Dataset S02 using the regular expression structures generated earlier for each of the three sentence structure types.\n",
    "\n",
    "Now we need to get the SMILES representation for each drug, which we can get from go.drugbank.com using the extracted DrugBank ID.\n",
    "\n",
    "Finally, we can merge the extracted Drug Bank IDs with the retrieved SMILES representations to create the full dataset.\n",
    "\n",
    "For the full implementation and details, see the GitHub project and preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "yp0VEr6H552O"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsgyJ0jvBT9y"
   },
   "source": [
    "The model's implementation is based on the ChemicalX implementation [2] and can be seen below. It takes in a feature vector (the size of which we can specify with `drug_feature_length`), and outputs the probabilities for each class (the number of which we specify with `num_categories`). We can also change the complexity of the model by setting `num_hidden_layers` and `hidden_dimension`. The model itself is a simple feed forward network with ReLU activations and batch normalization. A pretrained model with hyperparameters 2 layers and 128 hidden dimension can be found in the GitHub project as `2_layer_128_dim_model.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3FDc9m7w-Vhp"
   },
   "outputs": [],
   "source": [
    "class DeepDDI(nn.Module):\n",
    "    \"\"\"An implementation of the DeepDDI model from [ryu2018]_.\n",
    "\n",
    "    .. seealso:: This model was suggested in https://github.com/AstraZeneca/chemicalx/issues/2\n",
    "\n",
    "    .. [ryu2018] Ryu, J. Y., *et al.* (2018). `Deep learning improves prediction\n",
    "       of drugâ€“drug and drugâ€“food interactions <https://doi.org/10.1073/pnas.1803294115>`_.\n",
    "       *Proceedings of the National Academy of Sciences*, 115(18), E4304â€“E4311.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        drug_feature_length: int,\n",
    "        num_hidden_layers: int,\n",
    "        hidden_dimension: int,\n",
    "        num_categories: int,\n",
    "    ):\n",
    "        \"\"\"Instantiate the DeepDDI model.\n",
    "\n",
    "        :param drug_feature_length: The length of the input feature vector for an individual drug.\n",
    "        :param num_hidden_layers: The number of hidden layers.\n",
    "        :param hidden_dimension: The size of the hidden linear layer dimension.\n",
    "        :param num_categories: The number of output categories that the model predicts.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert num_hidden_layers > 1\n",
    "        layers = [\n",
    "            nn.Linear(drug_feature_length * 2, hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=hidden_dimension, affine=True, momentum=None),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.extend(\n",
    "                [\n",
    "                    nn.Linear(hidden_dimension, hidden_dimension),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(num_features=hidden_dimension, affine=True, momentum=None),\n",
    "                    nn.ReLU(),\n",
    "                ]\n",
    "            )\n",
    "        layers.extend([nn.Linear(hidden_dimension, num_categories), nn.Sigmoid()])\n",
    "        self.final = nn.Sequential(*layers)\n",
    "\n",
    "    def _combine_sides(self, left: torch.FloatTensor, right: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return torch.cat([left, right], dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        drug_features_a: torch.FloatTensor,\n",
    "        drug_features_b: torch.FloatTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"Run a forward pass of the DeepDDI model.\n",
    "\n",
    "        :param drug_features_a: feature vector for drug A.\n",
    "        :param drug_features_b: feature vector for drug B.\n",
    "        :returns: A column vector of predicted interaction scores.\n",
    "        \"\"\"\n",
    "        hidden = self._combine_sides(drug_features_a, drug_features_b)\n",
    "        return self.final(hidden)\n",
    "\n",
    "\n",
    "class DDIDataset(Dataset):\n",
    "    def __init__(self, csv_file, feature_size, num_categories=86):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.fpgen = AllChem.GetRDKitFPGenerator()\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        drug_a_smiles = row['drug_a_smiles']\n",
    "        drug_b_smiles = row['drug_b_smiles']\n",
    "        drug_a_id = row['drug_a']\n",
    "        drug_b_id = row['drug_b']\n",
    "        interaction = row['interaction']\n",
    "\n",
    "        interaction_tensor = torch.tensor(interaction, dtype=torch.int64)\n",
    "        interaction_tensor -= 1 # set indexing to start at 0\n",
    "        interaction_one_hot_label = nn.functional.one_hot(interaction_tensor, num_classes=num_categories)\n",
    "\n",
    "        drug_a_fp = self.get_fingerprint(drug_a_smiles)\n",
    "        drug_b_fp = self.get_fingerprint(drug_b_smiles)\n",
    "\n",
    "        drug_a_tensor = torch.tensor(drug_a_fp, dtype=torch.float32)\n",
    "        drug_b_tensor = torch.tensor(drug_b_fp, dtype=torch.float32)\n",
    "        label_tensor = interaction_one_hot_label.float()\n",
    "\n",
    "        return drug_a_tensor, drug_b_tensor, label_tensor\n",
    "\n",
    "    def get_fingerprint(self, smiles):\n",
    "        ms = Chem.MolFromSmiles(smiles)\n",
    "        if ms is not None:\n",
    "            fp = self.fpgen.GetFingerprint(ms)\n",
    "            array = np.zeros((self.feature_size,), dtype=np.float32)\n",
    "            DataStructs.ConvertToNumpyArray(fp, array)\n",
    "            return array\n",
    "        else:\n",
    "            return np.zeros((self.feature_size,), dtype=np.float32)\n",
    "\n",
    "\n",
    "def train_pca_model(ddi_dataset, n_components):\n",
    "    # Collect all drug fingerprints from the dataset\n",
    "    fingerprints = []\n",
    "    for i in range(len(ddi_dataset)):\n",
    "        drug_a_tensor, drug_b_tensor, _ = ddi_dataset[i]\n",
    "        fingerprints.append(drug_a_tensor.numpy())\n",
    "        fingerprints.append(drug_b_tensor.numpy())\n",
    "\n",
    "    # Convert the list of fingerprints to a numpy array\n",
    "    fingerprints_array = np.array(fingerprints)\n",
    "\n",
    "    # Create a PCA model\n",
    "    pca_model = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit the PCA model to the fingerprints\n",
    "    pca_model.fit(fingerprints_array)\n",
    "\n",
    "    return pca_model\n",
    "\n",
    "\n",
    "class DDIDatasetPCA(Dataset):\n",
    "    def __init__(self, ddi_dataset, pca_model):\n",
    "        self.ddi_dataset = ddi_dataset\n",
    "        self.pca_model = pca_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ddi_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        drug_a_tensor, drug_b_tensor, label_tensor = self.ddi_dataset[idx]\n",
    "\n",
    "        # Apply PCA dimensionality reduction to drug fingerprints\n",
    "        drug_a_reduced = self.apply_pca(drug_a_tensor)\n",
    "        drug_b_reduced = self.apply_pca(drug_b_tensor)\n",
    "\n",
    "        return drug_a_reduced, drug_b_reduced, label_tensor\n",
    "\n",
    "    def apply_pca(self, tensor):\n",
    "        # Convert tensor to numpy array\n",
    "        array = tensor.numpy()\n",
    "\n",
    "        # Reshape the array to 2D if necessary\n",
    "        if array.ndim == 1:\n",
    "            array = array.reshape(1, -1)\n",
    "\n",
    "        # Apply PCA transformation\n",
    "        reduced_array = self.pca_model.transform(array)\n",
    "\n",
    "        # Convert the reduced array back to a tensor\n",
    "        reduced_tensor = torch.from_numpy(reduced_array).float()\n",
    "\n",
    "        return reduced_tensor.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UwFoSKojNIn"
   },
   "source": [
    "*Training*\n",
    "\n",
    "We follow the original paper's hyperparameters for the learning rate (0.0001), batch size (256), and optimizer (Adam). The paper claimed that the 9 layer model with 2048 hidden dimension size gave the best performance on a hold-out test set. Therefore, we tested a similar configuration (8 layers, 2048 hidden dimension size) along with other models of varying complexity in the range of [2, 4, 8] layers and [128, 2048] hidden dimension size.\n",
    "\n",
    "\n",
    "Training a model takes on the order of 4-15 minutes for 5 epochs with 8,192 training samples, and in total we had 15 models. Therefore, the total GPU time with a single T4 was approximately 3-4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JA6B3YhsR3Mo",
    "outputId": "a12733a3-7828-4810-b8af-bd0ac2a10739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device}')\n",
    "\n",
    "# Set hyperparameters\n",
    "drug_feature_length = 2048\n",
    "num_hidden_layers = 2 # 2 4 8\n",
    "hidden_dimension = 2**7 # 2**7 2**11\n",
    "num_categories = 86\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Initialize the model\n",
    "model = DeepDDI(\n",
    "    drug_feature_length=drug_feature_length,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_dimension=hidden_dimension,\n",
    "    num_categories=num_categories,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mcB6238WHWv",
    "outputId": "47b4ef16-cbe3-40d8-96b5-1c61dcea3cb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 2**8\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare the dataset\n",
    "train_dataset = DDIDataset(\"/content/team64-project-cs598-dlh/data/train_ds.csv\",\n",
    "                     drug_feature_length,\n",
    "                     num_categories,\n",
    "                     )\n",
    "\n",
    "val_dataset = DDIDataset(\"/content/team64-project-cs598-dlh/data/val_ds.csv\",\n",
    "                     drug_feature_length,\n",
    "                     num_categories,\n",
    "                     )\n",
    "\n",
    "train_subset_size = 2**13 # len(train_dataset) # reduce for quicker training\n",
    "train_subset_indices = random.sample(range(len(train_dataset)), train_subset_size)\n",
    "train_subset_dataset = Subset(train_dataset, train_subset_indices)\n",
    "\n",
    "val_subset_size = 2**12 # len(val_dataset) # reduce for quicker training\n",
    "val_subset_indices = random.sample(range(len(val_dataset)), val_subset_size)\n",
    "val_subset_dataset = Subset(val_dataset, val_subset_indices)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "val_size = int(0.5 * len(val_subset_dataset))\n",
    "test_size = len(val_subset_dataset) - val_size\n",
    "\n",
    "val_dataset, test_dataset = random_split(val_subset_dataset, [val_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_subset_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nieAAIsCRqr"
   },
   "source": [
    "In the original paper, the feature representation is not the direct molecular fingerprint of the drug, but rather a dimensionality-reduced version created by using Principal Component Analysis on the training set. Here, we use the direct molecular fingerprint, which is a binary encoding of length 2048. For the draft, we reduce the complexity of the model compared to the paper (8 layers --> 3 layers), and we also limit the training by reducing the dataset size as well as the number of training epochs (100 to 5). Furthermore, we only predict a single DDI type, so the output is a single probability of interaction, whereas the original paper defines an output of 86 probabilities for each tested DDI type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwovu7IoWT1w",
    "outputId": "2a2dcceb-4a2e-4d72-a560-a710b60a8e4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2024-05-08 04:20:40.286331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [1/32], Training Loss: 4.4751\n",
      "Epoch [1/5], Batch [2/32], Training Loss: 4.4654\n",
      "Epoch [1/5], Batch [3/32], Training Loss: 4.4528\n",
      "Epoch [1/5], Batch [4/32], Training Loss: 4.4604\n",
      "Epoch [1/5], Batch [5/32], Training Loss: 4.4515\n",
      "Epoch [1/5], Batch [6/32], Training Loss: 4.4320\n",
      "Epoch [1/5], Batch [7/32], Training Loss: 4.4275\n",
      "Epoch [1/5], Batch [8/32], Training Loss: 4.4342\n",
      "Epoch [1/5], Batch [9/32], Training Loss: 4.4186\n",
      "Epoch [1/5], Batch [10/32], Training Loss: 4.4068\n",
      "Epoch [1/5], Batch [11/32], Training Loss: 4.4039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:21:07] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[04:21:07] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [12/32], Training Loss: 4.3998\n",
      "Epoch [1/5], Batch [13/32], Training Loss: 4.3950\n",
      "Epoch [1/5], Batch [14/32], Training Loss: 4.3940\n",
      "Epoch [1/5], Batch [15/32], Training Loss: 4.3756\n",
      "Epoch [1/5], Batch [16/32], Training Loss: 4.3859\n",
      "Epoch [1/5], Batch [17/32], Training Loss: 4.3730\n",
      "Epoch [1/5], Batch [18/32], Training Loss: 4.3639\n",
      "Epoch [1/5], Batch [19/32], Training Loss: 4.3705\n",
      "Epoch [1/5], Batch [20/32], Training Loss: 4.3596\n",
      "Epoch [1/5], Batch [21/32], Training Loss: 4.3578\n",
      "Epoch [1/5], Batch [22/32], Training Loss: 4.3518\n",
      "Epoch [1/5], Batch [23/32], Training Loss: 4.3525\n",
      "Epoch [1/5], Batch [24/32], Training Loss: 4.3377\n",
      "Epoch [1/5], Batch [25/32], Training Loss: 4.3423\n",
      "Epoch [1/5], Batch [26/32], Training Loss: 4.3456\n",
      "Epoch [1/5], Batch [27/32], Training Loss: 4.3381\n",
      "Epoch [1/5], Batch [28/32], Training Loss: 4.3246\n",
      "Epoch [1/5], Batch [29/32], Training Loss: 4.3186\n",
      "Epoch [1/5], Batch [30/32], Training Loss: 4.3147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [31/32], Training Loss: 4.3238\n",
      "Epoch [1/5], Batch [32/32], Training Loss: 4.3038\n",
      "Epoch [1/5], Average Training Loss: 4.3830\n",
      "Epoch [1/5], Batch [1/8], Validation Loss: 4.2725\n",
      "Epoch [1/5], Batch [2/8], Validation Loss: 4.2327\n",
      "Epoch [1/5], Batch [3/8], Validation Loss: 4.2424\n",
      "Epoch [1/5], Batch [4/8], Validation Loss: 4.2479\n",
      "Epoch [1/5], Batch [5/8], Validation Loss: 4.2401\n",
      "Epoch [1/5], Batch [6/8], Validation Loss: 4.2531\n",
      "Epoch [1/5], Batch [7/8], Validation Loss: 4.2493\n",
      "Epoch [1/5], Batch [8/8], Validation Loss: 4.2766\n",
      "Epoch [1/5], Average Validation Loss: 4.2518\n",
      "Epoch [2/5], Batch [1/32], Training Loss: 4.2816\n",
      "Epoch [2/5], Batch [2/32], Training Loss: 4.2791\n",
      "Epoch [2/5], Batch [3/32], Training Loss: 4.2910\n",
      "Epoch [2/5], Batch [4/32], Training Loss: 4.2638\n",
      "Epoch [2/5], Batch [5/32], Training Loss: 4.2702\n",
      "Epoch [2/5], Batch [6/32], Training Loss: 4.2668\n",
      "Epoch [2/5], Batch [7/32], Training Loss: 4.2542\n",
      "Epoch [2/5], Batch [8/32], Training Loss: 4.2572\n",
      "Epoch [2/5], Batch [9/32], Training Loss: 4.2422\n",
      "Epoch [2/5], Batch [10/32], Training Loss: 4.2514\n",
      "Epoch [2/5], Batch [11/32], Training Loss: 4.2552\n",
      "Epoch [2/5], Batch [12/32], Training Loss: 4.2475\n",
      "Epoch [2/5], Batch [13/32], Training Loss: 4.2429\n",
      "Epoch [2/5], Batch [14/32], Training Loss: 4.2481\n",
      "Epoch [2/5], Batch [15/32], Training Loss: 4.2374\n",
      "Epoch [2/5], Batch [16/32], Training Loss: 4.2342\n",
      "Epoch [2/5], Batch [17/32], Training Loss: 4.2440\n",
      "Epoch [2/5], Batch [18/32], Training Loss: 4.2210\n",
      "Epoch [2/5], Batch [19/32], Training Loss: 4.2163\n",
      "Epoch [2/5], Batch [20/32], Training Loss: 4.2366\n",
      "Epoch [2/5], Batch [21/32], Training Loss: 4.2270\n",
      "Epoch [2/5], Batch [22/32], Training Loss: 4.2392\n",
      "Epoch [2/5], Batch [23/32], Training Loss: 4.2046\n",
      "Epoch [2/5], Batch [24/32], Training Loss: 4.2105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:22:36] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[04:22:36] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Batch [25/32], Training Loss: 4.2091\n",
      "Epoch [2/5], Batch [26/32], Training Loss: 4.2177\n",
      "Epoch [2/5], Batch [27/32], Training Loss: 4.2069\n",
      "Epoch [2/5], Batch [28/32], Training Loss: 4.2030\n",
      "Epoch [2/5], Batch [29/32], Training Loss: 4.1939\n",
      "Epoch [2/5], Batch [30/32], Training Loss: 4.2089\n",
      "Epoch [2/5], Batch [31/32], Training Loss: 4.1869\n",
      "Epoch [2/5], Batch [32/32], Training Loss: 4.2043\n",
      "Epoch [2/5], Average Training Loss: 4.2360\n",
      "Epoch [2/5], Batch [1/8], Validation Loss: 4.2003\n",
      "Epoch [2/5], Batch [2/8], Validation Loss: 4.1800\n",
      "Epoch [2/5], Batch [3/8], Validation Loss: 4.1808\n",
      "Epoch [2/5], Batch [4/8], Validation Loss: 4.1857\n",
      "Epoch [2/5], Batch [5/8], Validation Loss: 4.1910\n",
      "Epoch [2/5], Batch [6/8], Validation Loss: 4.2082\n",
      "Epoch [2/5], Batch [7/8], Validation Loss: 4.1973\n",
      "Epoch [2/5], Batch [8/8], Validation Loss: 4.2151\n",
      "Epoch [2/5], Average Validation Loss: 4.1948\n",
      "Epoch [3/5], Batch [1/32], Training Loss: 4.1791\n",
      "Epoch [3/5], Batch [2/32], Training Loss: 4.1797\n",
      "Epoch [3/5], Batch [3/32], Training Loss: 4.1791\n",
      "Epoch [3/5], Batch [4/32], Training Loss: 4.1719\n",
      "Epoch [3/5], Batch [5/32], Training Loss: 4.1752\n",
      "Epoch [3/5], Batch [6/32], Training Loss: 4.1630\n",
      "Epoch [3/5], Batch [7/32], Training Loss: 4.1698\n",
      "Epoch [3/5], Batch [8/32], Training Loss: 4.1633\n",
      "Epoch [3/5], Batch [9/32], Training Loss: 4.1648\n",
      "Epoch [3/5], Batch [10/32], Training Loss: 4.1522\n",
      "Epoch [3/5], Batch [11/32], Training Loss: 4.1493\n",
      "Epoch [3/5], Batch [12/32], Training Loss: 4.1510\n",
      "Epoch [3/5], Batch [13/32], Training Loss: 4.1453\n",
      "Epoch [3/5], Batch [14/32], Training Loss: 4.1453\n",
      "Epoch [3/5], Batch [15/32], Training Loss: 4.1500\n",
      "Epoch [3/5], Batch [16/32], Training Loss: 4.1436\n",
      "Epoch [3/5], Batch [17/32], Training Loss: 4.1381\n",
      "Epoch [3/5], Batch [18/32], Training Loss: 4.1265\n",
      "Epoch [3/5], Batch [19/32], Training Loss: 4.1280\n",
      "Epoch [3/5], Batch [20/32], Training Loss: 4.1339\n",
      "Epoch [3/5], Batch [21/32], Training Loss: 4.1271\n",
      "Epoch [3/5], Batch [22/32], Training Loss: 4.1300\n",
      "Epoch [3/5], Batch [23/32], Training Loss: 4.1363\n",
      "Epoch [3/5], Batch [24/32], Training Loss: 4.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:23:28] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[04:23:28] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Batch [25/32], Training Loss: 4.1194\n",
      "Epoch [3/5], Batch [26/32], Training Loss: 4.1301\n",
      "Epoch [3/5], Batch [27/32], Training Loss: 4.1124\n",
      "Epoch [3/5], Batch [28/32], Training Loss: 4.1150\n",
      "Epoch [3/5], Batch [29/32], Training Loss: 4.1068\n",
      "Epoch [3/5], Batch [30/32], Training Loss: 4.1213\n",
      "Epoch [3/5], Batch [31/32], Training Loss: 4.1005\n",
      "Epoch [3/5], Batch [32/32], Training Loss: 4.1138\n",
      "Epoch [3/5], Average Training Loss: 4.1418\n",
      "Epoch [3/5], Batch [1/8], Validation Loss: 4.1319\n",
      "Epoch [3/5], Batch [2/8], Validation Loss: 4.1121\n",
      "Epoch [3/5], Batch [3/8], Validation Loss: 4.1179\n",
      "Epoch [3/5], Batch [4/8], Validation Loss: 4.1282\n",
      "Epoch [3/5], Batch [5/8], Validation Loss: 4.1373\n",
      "Epoch [3/5], Batch [6/8], Validation Loss: 4.1426\n",
      "Epoch [3/5], Batch [7/8], Validation Loss: 4.1338\n",
      "Epoch [3/5], Batch [8/8], Validation Loss: 4.1458\n",
      "Epoch [3/5], Average Validation Loss: 4.1312\n",
      "Epoch [4/5], Batch [1/32], Training Loss: 4.0906\n",
      "Epoch [4/5], Batch [2/32], Training Loss: 4.0991\n",
      "Epoch [4/5], Batch [3/32], Training Loss: 4.0872\n",
      "Epoch [4/5], Batch [4/32], Training Loss: 4.0890\n",
      "Epoch [4/5], Batch [5/32], Training Loss: 4.0803\n",
      "Epoch [4/5], Batch [6/32], Training Loss: 4.0820\n",
      "Epoch [4/5], Batch [7/32], Training Loss: 4.0869\n",
      "Epoch [4/5], Batch [8/32], Training Loss: 4.0867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:23:55] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[04:23:55] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Batch [9/32], Training Loss: 4.0838\n",
      "Epoch [4/5], Batch [10/32], Training Loss: 4.0634\n",
      "Epoch [4/5], Batch [11/32], Training Loss: 4.0759\n",
      "Epoch [4/5], Batch [12/32], Training Loss: 4.0751\n",
      "Epoch [4/5], Batch [13/32], Training Loss: 4.0553\n",
      "Epoch [4/5], Batch [14/32], Training Loss: 4.0783\n",
      "Epoch [4/5], Batch [15/32], Training Loss: 4.0758\n",
      "Epoch [4/5], Batch [16/32], Training Loss: 4.0550\n",
      "Epoch [4/5], Batch [17/32], Training Loss: 4.0706\n",
      "Epoch [4/5], Batch [18/32], Training Loss: 4.0763\n",
      "Epoch [4/5], Batch [19/32], Training Loss: 4.0679\n",
      "Epoch [4/5], Batch [20/32], Training Loss: 4.0623\n",
      "Epoch [4/5], Batch [21/32], Training Loss: 4.0616\n",
      "Epoch [4/5], Batch [22/32], Training Loss: 4.0592\n",
      "Epoch [4/5], Batch [23/32], Training Loss: 4.0590\n",
      "Epoch [4/5], Batch [24/32], Training Loss: 4.0540\n",
      "Epoch [4/5], Batch [25/32], Training Loss: 4.0594\n",
      "Epoch [4/5], Batch [26/32], Training Loss: 4.0562\n",
      "Epoch [4/5], Batch [27/32], Training Loss: 4.0517\n",
      "Epoch [4/5], Batch [28/32], Training Loss: 4.0655\n",
      "Epoch [4/5], Batch [29/32], Training Loss: 4.0516\n",
      "Epoch [4/5], Batch [30/32], Training Loss: 4.0540\n",
      "Epoch [4/5], Batch [31/32], Training Loss: 4.0591\n",
      "Epoch [4/5], Batch [32/32], Training Loss: 4.0470\n",
      "Epoch [4/5], Average Training Loss: 4.0694\n",
      "Epoch [4/5], Batch [1/8], Validation Loss: 4.0958\n",
      "Epoch [4/5], Batch [2/8], Validation Loss: 4.0708\n",
      "Epoch [4/5], Batch [3/8], Validation Loss: 4.0779\n",
      "Epoch [4/5], Batch [4/8], Validation Loss: 4.0853\n",
      "Epoch [4/5], Batch [5/8], Validation Loss: 4.0948\n",
      "Epoch [4/5], Batch [6/8], Validation Loss: 4.0999\n",
      "Epoch [4/5], Batch [7/8], Validation Loss: 4.0935\n",
      "Epoch [4/5], Batch [8/8], Validation Loss: 4.1027\n",
      "Epoch [4/5], Average Validation Loss: 4.0901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:24:48] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[04:24:48] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Batch [1/32], Training Loss: 4.0335\n",
      "Epoch [5/5], Batch [2/32], Training Loss: 4.0448\n",
      "Epoch [5/5], Batch [3/32], Training Loss: 4.0217\n",
      "Epoch [5/5], Batch [4/32], Training Loss: 4.0177\n",
      "Epoch [5/5], Batch [5/32], Training Loss: 4.0285\n",
      "Epoch [5/5], Batch [6/32], Training Loss: 4.0271\n",
      "Epoch [5/5], Batch [7/32], Training Loss: 4.0051\n",
      "Epoch [5/5], Batch [8/32], Training Loss: 4.0164\n",
      "Epoch [5/5], Batch [9/32], Training Loss: 4.0210\n",
      "Epoch [5/5], Batch [10/32], Training Loss: 4.0184\n",
      "Epoch [5/5], Batch [11/32], Training Loss: 4.0127\n",
      "Epoch [5/5], Batch [12/32], Training Loss: 4.0220\n",
      "Epoch [5/5], Batch [13/32], Training Loss: 4.0201\n",
      "Epoch [5/5], Batch [14/32], Training Loss: 4.0129\n",
      "Epoch [5/5], Batch [15/32], Training Loss: 4.0139\n",
      "Epoch [5/5], Batch [16/32], Training Loss: 4.0068\n",
      "Epoch [5/5], Batch [17/32], Training Loss: 4.0037\n",
      "Epoch [5/5], Batch [18/32], Training Loss: 4.0178\n",
      "Epoch [5/5], Batch [19/32], Training Loss: 4.0093\n",
      "Epoch [5/5], Batch [20/32], Training Loss: 4.0125\n",
      "Epoch [5/5], Batch [21/32], Training Loss: 4.0115\n",
      "Epoch [5/5], Batch [22/32], Training Loss: 4.0041\n",
      "Epoch [5/5], Batch [23/32], Training Loss: 3.9962\n",
      "Epoch [5/5], Batch [24/32], Training Loss: 3.9970\n",
      "Epoch [5/5], Batch [25/32], Training Loss: 3.9938\n",
      "Epoch [5/5], Batch [26/32], Training Loss: 3.9911\n",
      "Epoch [5/5], Batch [27/32], Training Loss: 3.9940\n",
      "Epoch [5/5], Batch [28/32], Training Loss: 3.9962\n",
      "Epoch [5/5], Batch [29/32], Training Loss: 3.9917\n",
      "Epoch [5/5], Batch [30/32], Training Loss: 3.9932\n",
      "Epoch [5/5], Batch [31/32], Training Loss: 3.9879\n",
      "Epoch [5/5], Batch [32/32], Training Loss: 3.9920\n",
      "Epoch [5/5], Average Training Loss: 4.0098\n",
      "Epoch [5/5], Batch [1/8], Validation Loss: 4.0429\n",
      "Epoch [5/5], Batch [2/8], Validation Loss: 4.0184\n",
      "Epoch [5/5], Batch [3/8], Validation Loss: 4.0200\n",
      "Epoch [5/5], Batch [4/8], Validation Loss: 4.0352\n",
      "Epoch [5/5], Batch [5/8], Validation Loss: 4.0473\n",
      "Epoch [5/5], Batch [6/8], Validation Loss: 4.0437\n",
      "Epoch [5/5], Batch [7/8], Validation Loss: 4.0443\n",
      "Epoch [5/5], Batch [8/8], Validation Loss: 4.0466\n",
      "Epoch [5/5], Average Validation Loss: 4.0373\n",
      "Training finished! Training took 0:05:02.993448 (2024-05-08 04:20:40.286331 to 2024-05-08 04:25:43.279779)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "start = datetime.now()\n",
    "print(f'Starting at {start}')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        inputs_left, inputs_right, labels = data\n",
    "        inputs_left, inputs_right = inputs_left.to(device), inputs_right.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs_left, inputs_right)\n",
    "        inputs_left, inputs_right = inputs_left.cpu(), inputs_right.cpu()\n",
    "        labels = labels.to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        labels = labels.cpu()\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print the loss for each batch\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_dataloader)}], Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            inputs_left, inputs_right, labels = data\n",
    "            inputs_left, inputs_right = inputs_left.to(device), inputs_right.to(device)\n",
    "            outputs = model(inputs_left, inputs_right)\n",
    "            inputs_left, inputs_right = inputs_left.cpu(), inputs_right.cpu()\n",
    "            labels = labels.to(device)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            labels = labels.cpu()\n",
    "            val_loss += loss.item()\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(val_dataloader)}], Validation Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Validation Loss: {val_loss:.4f}\")\n",
    "    model.train()\n",
    "end = datetime.now()\n",
    "print(f\"Training finished! Training took {end - start} ({start} to {end})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWGa8ZC_Y6ZO"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./{num_hidden_layers}_layer_{hidden_dimension}_dim_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73sFEJphEAQI"
   },
   "source": [
    "With the current configuration, we get a test set accuracy of over 80%, with this specific model checkpoint achieving 82.8% accuracy with the particular random seed set. The original paper only measured overall accuracy for each DDI type, so we reflect that by only measuring the accuracy for an evaluation metric. The original paper also used 0.47 as a cutoff for prediction that there is a drug-drug interaction, so we also modified this from standard practice of using 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uU8COAHP7vo3",
    "outputId": "3c00390e-a53c-40a2-cbda-3cc5ca550beb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'./{num_hidden_layers}_layer_{hidden_dimension}_dim_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLGE-LUYmcti"
   },
   "source": [
    "*Evaluation*\n",
    "\n",
    "We follow the same metric for evaluation as the original paper by recording the average accuracy across each of the 86 DDI types. The original paper did not note if the averages were weighted, so we assumed a simple average of the accuracies. The implementation is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSaMB9OsWQrC",
    "outputId": "6d963bf8-17eb-414d-e720-35a790099ce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDI 1 accuracy: 0.966796875\n",
      "DDI 2 accuracy: 0.85302734375\n",
      "DDI 3 accuracy: 0.83154296875\n",
      "DDI 4 accuracy: 0.97509765625\n",
      "DDI 5 accuracy: 0.87744140625\n",
      "DDI 6 accuracy: 0.390625\n",
      "DDI 7 accuracy: 0.57763671875\n",
      "DDI 8 accuracy: 0.921875\n",
      "DDI 9 accuracy: 0.078125\n",
      "DDI 10 accuracy: 0.3935546875\n",
      "DDI 11 accuracy: 0.80517578125\n",
      "DDI 12 accuracy: 0.892578125\n",
      "DDI 13 accuracy: 0.166015625\n",
      "DDI 14 accuracy: 0.83056640625\n",
      "DDI 15 accuracy: 0.87548828125\n",
      "DDI 16 accuracy: 0.96484375\n",
      "DDI 17 accuracy: 0.70751953125\n",
      "DDI 18 accuracy: 0.95703125\n",
      "DDI 19 accuracy: 0.966796875\n",
      "DDI 20 accuracy: 0.96728515625\n",
      "DDI 21 accuracy: 0.64453125\n",
      "DDI 22 accuracy: 0.92138671875\n",
      "DDI 23 accuracy: 0.69677734375\n",
      "DDI 24 accuracy: 0.96875\n",
      "DDI 25 accuracy: 0.9970703125\n",
      "DDI 26 accuracy: 0.515625\n",
      "DDI 27 accuracy: 0.96240234375\n",
      "DDI 28 accuracy: 0.97607421875\n",
      "DDI 29 accuracy: 0.80810546875\n",
      "DDI 30 accuracy: 0.869140625\n",
      "DDI 31 accuracy: 0.93212890625\n",
      "DDI 32 accuracy: 0.94921875\n",
      "DDI 33 accuracy: 0.9501953125\n",
      "DDI 34 accuracy: 0.84814453125\n",
      "DDI 35 accuracy: 0.73779296875\n",
      "DDI 36 accuracy: 0.99609375\n",
      "DDI 37 accuracy: 0.9619140625\n",
      "DDI 38 accuracy: 0.662109375\n",
      "DDI 39 accuracy: 0.875\n",
      "DDI 40 accuracy: 0.97216796875\n",
      "DDI 41 accuracy: 0.92626953125\n",
      "DDI 42 accuracy: 0.98388671875\n",
      "DDI 43 accuracy: 0.9716796875\n",
      "DDI 44 accuracy: 0.80908203125\n",
      "DDI 45 accuracy: 0.84130859375\n",
      "DDI 46 accuracy: 0.50927734375\n",
      "DDI 47 accuracy: 0.72021484375\n",
      "DDI 48 accuracy: 0.7744140625\n",
      "DDI 49 accuracy: 0.962890625\n",
      "DDI 50 accuracy: 0.796875\n",
      "DDI 51 accuracy: 0.9775390625\n",
      "DDI 52 accuracy: 0.52294921875\n",
      "DDI 53 accuracy: 0.95703125\n",
      "DDI 54 accuracy: 0.87158203125\n",
      "DDI 55 accuracy: 0.9931640625\n",
      "DDI 56 accuracy: 0.927734375\n",
      "DDI 57 accuracy: 0.67822265625\n",
      "DDI 58 accuracy: 0.970703125\n",
      "DDI 59 accuracy: 0.84423828125\n",
      "DDI 60 accuracy: 0.97265625\n",
      "DDI 61 accuracy: 0.642578125\n",
      "DDI 62 accuracy: 0.98095703125\n",
      "DDI 63 accuracy: 0.96826171875\n",
      "DDI 64 accuracy: 0.62939453125\n",
      "DDI 65 accuracy: 0.833984375\n",
      "DDI 66 accuracy: 0.75341796875\n",
      "DDI 67 accuracy: 0.46533203125\n",
      "DDI 68 accuracy: 0.970703125\n",
      "DDI 69 accuracy: 0.80517578125\n",
      "DDI 70 accuracy: 0.8369140625\n",
      "DDI 71 accuracy: 0.923828125\n",
      "DDI 72 accuracy: 0.5791015625\n",
      "DDI 73 accuracy: 0.98291015625\n",
      "DDI 74 accuracy: 0.7900390625\n",
      "DDI 75 accuracy: 0.65087890625\n",
      "DDI 76 accuracy: 0.314453125\n",
      "DDI 77 accuracy: 0.9287109375\n",
      "DDI 78 accuracy: 0.65966796875\n",
      "DDI 79 accuracy: 0.8544921875\n",
      "DDI 80 accuracy: 0.97119140625\n",
      "DDI 81 accuracy: 0.86279296875\n",
      "DDI 82 accuracy: 0.88427734375\n",
      "DDI 83 accuracy: 0.89013671875\n",
      "DDI 84 accuracy: 0.94384765625\n",
      "DDI 85 accuracy: 0.9814453125\n",
      "DDI 86 accuracy: 0.98583984375\n"
     ]
    }
   ],
   "source": [
    "# Test set evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs_left, inputs_right, labels = data\n",
    "        inputs_left, inputs_right = inputs_left.to(device), inputs_right.to(device)\n",
    "\n",
    "        outputs = model(inputs_left, inputs_right)\n",
    "        predicted_labels = (outputs.squeeze() > 0.47).float()\n",
    "        predictions.extend(predicted_labels.tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "ddi_accuracies = {}\n",
    "for ddi_type in range(len(true_labels[0])):\n",
    "  ddi_true = [true_label[ddi_type] for true_label in true_labels]\n",
    "  ddi_pred = [pred_label[ddi_type] for pred_label in predictions]\n",
    "  ddi_accuracy = accuracy_score(ddi_true, ddi_pred)\n",
    "  ddi_accuracies[ddi_type] = ddi_accuracy\n",
    "  print(f'DDI {ddi_type+1} accuracy: {ddi_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gxx38i-cNv-X",
    "outputId": "863dcb06-063e-4726-b92f-3a035e8c2d0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8179732921511628"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(ddi_accuracies.values()))/len(ddi_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dmGNUZVwxv3P",
    "outputId": "110841f2-a13f-4aac-8ba8-851640e26718"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:51:22] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[04:51:22] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    }
   ],
   "source": [
    "n_components = 30  # Specify the desired number of components\n",
    "pca_model = train_pca_model(train_subset_dataset, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpSIycm0zMfW"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = DeepDDI(\n",
    "    drug_feature_length=n_components,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_dimension=hidden_dimension,\n",
    "    num_categories=num_categories,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create an instance of DDIDatasetPCA\n",
    "pca_train_dataset = DDIDatasetPCA(train_subset_dataset, pca_model)\n",
    "pca_val_dataset = DDIDatasetPCA(val_dataset, pca_model)\n",
    "pca_test_dataset = DDIDatasetPCA(test_dataset, pca_model)\n",
    "\n",
    "train_dataloader = DataLoader(pca_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(pca_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(pca_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSxMvS72zRS0",
    "outputId": "6393565d-f805-4cf7-9832-86ee0f9f14ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2024-05-08 04:51:59.569979\n",
      "Epoch [1/5], Batch [1/32], Training Loss: 4.4333\n",
      "Epoch [1/5], Batch [2/32], Training Loss: 4.4262\n",
      "Epoch [1/5], Batch [3/32], Training Loss: 4.4174\n",
      "Epoch [1/5], Batch [4/32], Training Loss: 4.4182\n",
      "Epoch [1/5], Batch [5/32], Training Loss: 4.4320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:52:10] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[04:52:10] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [6/32], Training Loss: 4.4139\n",
      "Epoch [1/5], Batch [7/32], Training Loss: 4.4084\n",
      "Epoch [1/5], Batch [8/32], Training Loss: 4.4103\n",
      "Epoch [1/5], Batch [9/32], Training Loss: 4.4046\n",
      "Epoch [1/5], Batch [10/32], Training Loss: 4.4115\n",
      "Epoch [1/5], Batch [11/32], Training Loss: 4.4050\n",
      "Epoch [1/5], Batch [12/32], Training Loss: 4.4058\n",
      "Epoch [1/5], Batch [13/32], Training Loss: 4.4019\n",
      "Epoch [1/5], Batch [14/32], Training Loss: 4.3963\n",
      "Epoch [1/5], Batch [15/32], Training Loss: 4.3886\n",
      "Epoch [1/5], Batch [16/32], Training Loss: 4.3874\n",
      "Epoch [1/5], Batch [17/32], Training Loss: 4.3847\n",
      "Epoch [1/5], Batch [18/32], Training Loss: 4.3882\n",
      "Epoch [1/5], Batch [19/32], Training Loss: 4.3888\n",
      "Epoch [1/5], Batch [20/32], Training Loss: 4.3761\n",
      "Epoch [1/5], Batch [21/32], Training Loss: 4.3905\n",
      "Epoch [1/5], Batch [22/32], Training Loss: 4.3808\n",
      "Epoch [1/5], Batch [23/32], Training Loss: 4.3738\n",
      "Epoch [1/5], Batch [24/32], Training Loss: 4.3832\n",
      "Epoch [1/5], Batch [25/32], Training Loss: 4.3703\n",
      "Epoch [1/5], Batch [26/32], Training Loss: 4.3786\n",
      "Epoch [1/5], Batch [27/32], Training Loss: 4.3612\n",
      "Epoch [1/5], Batch [28/32], Training Loss: 4.3645\n",
      "Epoch [1/5], Batch [29/32], Training Loss: 4.3625\n",
      "Epoch [1/5], Batch [30/32], Training Loss: 4.3596\n",
      "Epoch [1/5], Batch [31/32], Training Loss: 4.3561\n",
      "Epoch [1/5], Batch [32/32], Training Loss: 4.3604\n",
      "Epoch [1/5], Average Training Loss: 4.3919\n",
      "Epoch [1/5], Batch [1/8], Validation Loss: 4.3638\n",
      "Epoch [1/5], Batch [2/8], Validation Loss: 4.3372\n",
      "Epoch [1/5], Batch [3/8], Validation Loss: 4.3353\n",
      "Epoch [1/5], Batch [4/8], Validation Loss: 4.3474\n",
      "Epoch [1/5], Batch [5/8], Validation Loss: 4.3511\n",
      "Epoch [1/5], Batch [6/8], Validation Loss: 4.3502\n",
      "Epoch [1/5], Batch [7/8], Validation Loss: 4.3487\n",
      "Epoch [1/5], Batch [8/8], Validation Loss: 4.3517\n",
      "Epoch [1/5], Average Validation Loss: 4.3482\n",
      "Epoch [2/5], Batch [1/32], Training Loss: 4.3652\n",
      "Epoch [2/5], Batch [2/32], Training Loss: 4.3458\n",
      "Epoch [2/5], Batch [3/32], Training Loss: 4.3433\n",
      "Epoch [2/5], Batch [4/32], Training Loss: 4.3390\n",
      "Epoch [2/5], Batch [5/32], Training Loss: 4.3409\n",
      "Epoch [2/5], Batch [6/32], Training Loss: 4.3478\n",
      "Epoch [2/5], Batch [7/32], Training Loss: 4.3456\n",
      "Epoch [2/5], Batch [8/32], Training Loss: 4.3344\n",
      "Epoch [2/5], Batch [9/32], Training Loss: 4.3409\n",
      "Epoch [2/5], Batch [10/32], Training Loss: 4.3381\n",
      "Epoch [2/5], Batch [11/32], Training Loss: 4.3413\n",
      "Epoch [2/5], Batch [12/32], Training Loss: 4.3314\n",
      "Epoch [2/5], Batch [13/32], Training Loss: 4.3317\n",
      "Epoch [2/5], Batch [14/32], Training Loss: 4.3096\n",
      "Epoch [2/5], Batch [15/32], Training Loss: 4.3101\n",
      "Epoch [2/5], Batch [16/32], Training Loss: 4.3249\n",
      "Epoch [2/5], Batch [17/32], Training Loss: 4.3177\n",
      "Epoch [2/5], Batch [18/32], Training Loss: 4.3177\n",
      "Epoch [2/5], Batch [19/32], Training Loss: 4.3231\n",
      "Epoch [2/5], Batch [20/32], Training Loss: 4.3276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:54:58] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[04:54:58] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Batch [21/32], Training Loss: 4.3188\n",
      "Epoch [2/5], Batch [22/32], Training Loss: 4.2948\n",
      "Epoch [2/5], Batch [23/32], Training Loss: 4.3068\n",
      "Epoch [2/5], Batch [24/32], Training Loss: 4.3046\n",
      "Epoch [2/5], Batch [25/32], Training Loss: 4.3018\n",
      "Epoch [2/5], Batch [26/32], Training Loss: 4.2935\n",
      "Epoch [2/5], Batch [27/32], Training Loss: 4.3036\n",
      "Epoch [2/5], Batch [28/32], Training Loss: 4.2966\n",
      "Epoch [2/5], Batch [29/32], Training Loss: 4.2981\n",
      "Epoch [2/5], Batch [30/32], Training Loss: 4.2935\n",
      "Epoch [2/5], Batch [31/32], Training Loss: 4.2835\n",
      "Epoch [2/5], Batch [32/32], Training Loss: 4.2955\n",
      "Epoch [2/5], Average Training Loss: 4.3208\n",
      "Epoch [2/5], Batch [1/8], Validation Loss: 4.2926\n",
      "Epoch [2/5], Batch [2/8], Validation Loss: 4.2610\n",
      "Epoch [2/5], Batch [3/8], Validation Loss: 4.2617\n",
      "Epoch [2/5], Batch [4/8], Validation Loss: 4.2776\n",
      "Epoch [2/5], Batch [5/8], Validation Loss: 4.2830\n",
      "Epoch [2/5], Batch [6/8], Validation Loss: 4.2799\n",
      "Epoch [2/5], Batch [7/8], Validation Loss: 4.2764\n",
      "Epoch [2/5], Batch [8/8], Validation Loss: 4.2833\n",
      "Epoch [2/5], Average Validation Loss: 4.2769\n",
      "Epoch [3/5], Batch [1/32], Training Loss: 4.2947\n",
      "Epoch [3/5], Batch [2/32], Training Loss: 4.2924\n",
      "Epoch [3/5], Batch [3/32], Training Loss: 4.2856\n",
      "Epoch [3/5], Batch [4/32], Training Loss: 4.2753\n",
      "Epoch [3/5], Batch [5/32], Training Loss: 4.2869\n",
      "Epoch [3/5], Batch [6/32], Training Loss: 4.2634\n",
      "Epoch [3/5], Batch [7/32], Training Loss: 4.2754\n",
      "Epoch [3/5], Batch [8/32], Training Loss: 4.2570\n",
      "Epoch [3/5], Batch [9/32], Training Loss: 4.2841\n",
      "Epoch [3/5], Batch [10/32], Training Loss: 4.2777\n",
      "Epoch [3/5], Batch [11/32], Training Loss: 4.2657\n",
      "Epoch [3/5], Batch [12/32], Training Loss: 4.2604\n",
      "Epoch [3/5], Batch [13/32], Training Loss: 4.2664\n",
      "Epoch [3/5], Batch [14/32], Training Loss: 4.2838\n",
      "Epoch [3/5], Batch [15/32], Training Loss: 4.2603\n",
      "Epoch [3/5], Batch [16/32], Training Loss: 4.2573\n",
      "Epoch [3/5], Batch [17/32], Training Loss: 4.2612\n",
      "Epoch [3/5], Batch [18/32], Training Loss: 4.2482\n",
      "Epoch [3/5], Batch [19/32], Training Loss: 4.2381\n",
      "Epoch [3/5], Batch [20/32], Training Loss: 4.2564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:57:09] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[04:57:09] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Batch [21/32], Training Loss: 4.2354\n",
      "Epoch [3/5], Batch [22/32], Training Loss: 4.2441\n",
      "Epoch [3/5], Batch [23/32], Training Loss: 4.2446\n",
      "Epoch [3/5], Batch [24/32], Training Loss: 4.2536\n",
      "Epoch [3/5], Batch [25/32], Training Loss: 4.2459\n",
      "Epoch [3/5], Batch [26/32], Training Loss: 4.2344\n",
      "Epoch [3/5], Batch [27/32], Training Loss: 4.2430\n",
      "Epoch [3/5], Batch [28/32], Training Loss: 4.2425\n",
      "Epoch [3/5], Batch [29/32], Training Loss: 4.2212\n",
      "Epoch [3/5], Batch [30/32], Training Loss: 4.2334\n",
      "Epoch [3/5], Batch [31/32], Training Loss: 4.2324\n",
      "Epoch [3/5], Batch [32/32], Training Loss: 4.2332\n",
      "Epoch [3/5], Average Training Loss: 4.2579\n",
      "Epoch [3/5], Batch [1/8], Validation Loss: 4.2288\n",
      "Epoch [3/5], Batch [2/8], Validation Loss: 4.1943\n",
      "Epoch [3/5], Batch [3/8], Validation Loss: 4.1971\n",
      "Epoch [3/5], Batch [4/8], Validation Loss: 4.2168\n",
      "Epoch [3/5], Batch [5/8], Validation Loss: 4.2226\n",
      "Epoch [3/5], Batch [6/8], Validation Loss: 4.2172\n",
      "Epoch [3/5], Batch [7/8], Validation Loss: 4.2115\n",
      "Epoch [3/5], Batch [8/8], Validation Loss: 4.2229\n",
      "Epoch [3/5], Average Validation Loss: 4.2139\n",
      "Epoch [4/5], Batch [1/32], Training Loss: 4.2359\n",
      "Epoch [4/5], Batch [2/32], Training Loss: 4.2195\n",
      "Epoch [4/5], Batch [3/32], Training Loss: 4.2089\n",
      "Epoch [4/5], Batch [4/32], Training Loss: 4.2226\n",
      "Epoch [4/5], Batch [5/32], Training Loss: 4.2227\n",
      "Epoch [4/5], Batch [6/32], Training Loss: 4.2144\n",
      "Epoch [4/5], Batch [7/32], Training Loss: 4.2180\n",
      "Epoch [4/5], Batch [8/32], Training Loss: 4.2280\n",
      "Epoch [4/5], Batch [9/32], Training Loss: 4.2052\n",
      "Epoch [4/5], Batch [10/32], Training Loss: 4.2159\n",
      "Epoch [4/5], Batch [11/32], Training Loss: 4.2153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:58:41] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[04:58:41] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Batch [12/32], Training Loss: 4.2210\n",
      "Epoch [4/5], Batch [13/32], Training Loss: 4.2149\n",
      "Epoch [4/5], Batch [14/32], Training Loss: 4.2138\n",
      "Epoch [4/5], Batch [15/32], Training Loss: 4.2069\n",
      "Epoch [4/5], Batch [16/32], Training Loss: 4.1994\n",
      "Epoch [4/5], Batch [17/32], Training Loss: 4.2039\n",
      "Epoch [4/5], Batch [18/32], Training Loss: 4.1955\n",
      "Epoch [4/5], Batch [19/32], Training Loss: 4.1782\n",
      "Epoch [4/5], Batch [20/32], Training Loss: 4.1931\n",
      "Epoch [4/5], Batch [21/32], Training Loss: 4.2054\n",
      "Epoch [4/5], Batch [22/32], Training Loss: 4.1892\n",
      "Epoch [4/5], Batch [23/32], Training Loss: 4.1892\n",
      "Epoch [4/5], Batch [24/32], Training Loss: 4.1786\n",
      "Epoch [4/5], Batch [25/32], Training Loss: 4.1886\n",
      "Epoch [4/5], Batch [26/32], Training Loss: 4.1697\n",
      "Epoch [4/5], Batch [27/32], Training Loss: 4.1772\n",
      "Epoch [4/5], Batch [28/32], Training Loss: 4.1979\n",
      "Epoch [4/5], Batch [29/32], Training Loss: 4.1925\n",
      "Epoch [4/5], Batch [30/32], Training Loss: 4.1921\n",
      "Epoch [4/5], Batch [31/32], Training Loss: 4.1730\n",
      "Epoch [4/5], Batch [32/32], Training Loss: 4.1648\n",
      "Epoch [4/5], Average Training Loss: 4.2016\n",
      "Epoch [4/5], Batch [1/8], Validation Loss: 4.1711\n",
      "Epoch [4/5], Batch [2/8], Validation Loss: 4.1353\n",
      "Epoch [4/5], Batch [3/8], Validation Loss: 4.1397\n",
      "Epoch [4/5], Batch [4/8], Validation Loss: 4.1629\n",
      "Epoch [4/5], Batch [5/8], Validation Loss: 4.1687\n",
      "Epoch [4/5], Batch [6/8], Validation Loss: 4.1612\n",
      "Epoch [4/5], Batch [7/8], Validation Loss: 4.1532\n",
      "Epoch [4/5], Batch [8/8], Validation Loss: 4.1681\n",
      "Epoch [4/5], Average Validation Loss: 4.1575\n",
      "Epoch [5/5], Batch [1/32], Training Loss: 4.1672\n",
      "Epoch [5/5], Batch [2/32], Training Loss: 4.1625\n",
      "Epoch [5/5], Batch [3/32], Training Loss: 4.1889\n",
      "Epoch [5/5], Batch [4/32], Training Loss: 4.1724\n",
      "Epoch [5/5], Batch [5/32], Training Loss: 4.1790\n",
      "Epoch [5/5], Batch [6/32], Training Loss: 4.1855\n",
      "Epoch [5/5], Batch [7/32], Training Loss: 4.1584\n",
      "Epoch [5/5], Batch [8/32], Training Loss: 4.1608\n",
      "Epoch [5/5], Batch [9/32], Training Loss: 4.1663\n",
      "Epoch [5/5], Batch [10/32], Training Loss: 4.1345\n",
      "Epoch [5/5], Batch [11/32], Training Loss: 4.1451\n",
      "Epoch [5/5], Batch [12/32], Training Loss: 4.1592\n",
      "Epoch [5/5], Batch [13/32], Training Loss: 4.1381\n",
      "Epoch [5/5], Batch [14/32], Training Loss: 4.1556\n",
      "Epoch [5/5], Batch [15/32], Training Loss: 4.1487\n",
      "Epoch [5/5], Batch [16/32], Training Loss: 4.1548\n",
      "Epoch [5/5], Batch [17/32], Training Loss: 4.1466\n",
      "Epoch [5/5], Batch [18/32], Training Loss: 4.1570\n",
      "Epoch [5/5], Batch [19/32], Training Loss: 4.1304\n",
      "Epoch [5/5], Batch [20/32], Training Loss: 4.1351\n",
      "Epoch [5/5], Batch [21/32], Training Loss: 4.1495\n",
      "Epoch [5/5], Batch [22/32], Training Loss: 4.1332\n",
      "Epoch [5/5], Batch [23/32], Training Loss: 4.1505\n",
      "Epoch [5/5], Batch [24/32], Training Loss: 4.1541\n",
      "Epoch [5/5], Batch [25/32], Training Loss: 4.1460\n",
      "Epoch [5/5], Batch [26/32], Training Loss: 4.1348\n",
      "Epoch [5/5], Batch [27/32], Training Loss: 4.1362\n",
      "Epoch [5/5], Batch [28/32], Training Loss: 4.1427\n",
      "Epoch [5/5], Batch [29/32], Training Loss: 4.1514\n",
      "Epoch [5/5], Batch [30/32], Training Loss: 4.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05:01:35] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[05:01:35] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Batch [31/32], Training Loss: 4.1335\n",
      "Epoch [5/5], Batch [32/32], Training Loss: 4.1149\n",
      "Epoch [5/5], Average Training Loss: 4.1505\n",
      "Epoch [5/5], Batch [1/8], Validation Loss: 4.1186\n",
      "Epoch [5/5], Batch [2/8], Validation Loss: 4.0824\n",
      "Epoch [5/5], Batch [3/8], Validation Loss: 4.0879\n",
      "Epoch [5/5], Batch [4/8], Validation Loss: 4.1150\n",
      "Epoch [5/5], Batch [5/8], Validation Loss: 4.1204\n",
      "Epoch [5/5], Batch [6/8], Validation Loss: 4.1109\n",
      "Epoch [5/5], Batch [7/8], Validation Loss: 4.1008\n",
      "Epoch [5/5], Batch [8/8], Validation Loss: 4.1178\n",
      "Epoch [5/5], Average Validation Loss: 4.1067\n",
      "Training finished! Training took 0:10:06.018729 (2024-05-08 04:51:59.569979 to 2024-05-08 05:02:05.588708)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "start = datetime.now()\n",
    "print(f'Starting at {start}')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        inputs_left, inputs_right, labels = data\n",
    "        inputs_left, inputs_right = inputs_left.to(device), inputs_right.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "\n",
    "        outputs = model(inputs_left, inputs_right)\n",
    "        inputs_left, inputs_right = inputs_left.cpu(), inputs_right.cpu()\n",
    "        labels = labels.to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        labels = labels.cpu()\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print the loss for each batch\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_dataloader)}], Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            inputs_left, inputs_right, labels = data\n",
    "            inputs_left, inputs_right = inputs_left.to(device), inputs_right.to(device)\n",
    "            outputs = model(inputs_left, inputs_right)\n",
    "            inputs_left, inputs_right = inputs_left.cpu(), inputs_right.cpu()\n",
    "            labels = labels.to(device)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            labels = labels.cpu()\n",
    "            val_loss += loss.item()\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(val_dataloader)}], Validation Loss: {loss.item():.4f}\")\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Validation Loss: {val_loss:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"Training finished! Training took {end - start} ({start} to {end})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzUqTqB-Ap7U"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./{n_components}_pca_{num_hidden_layers}_layer_{hidden_dimension}_dim_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdNIyGcIPDoZ",
    "outputId": "85b80879-be29-411f-b90c-9016abe15c4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'./{n_components}_pca_{num_hidden_layers}_layer_{hidden_dimension}_dim_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGZUurFfz4xi",
    "outputId": "3baab66f-34bb-4740-b6af-abbff31cb9cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDI 1 accuracy: 0.75244140625\n",
      "DDI 2 accuracy: 0.49072265625\n",
      "DDI 3 accuracy: 0.79541015625\n",
      "DDI 4 accuracy: 0.701171875\n",
      "DDI 5 accuracy: 0.89697265625\n",
      "DDI 6 accuracy: 0.20556640625\n",
      "DDI 7 accuracy: 0.19091796875\n",
      "DDI 8 accuracy: 0.94287109375\n",
      "DDI 9 accuracy: 0.09326171875\n",
      "DDI 10 accuracy: 0.19677734375\n",
      "DDI 11 accuracy: 0.8818359375\n",
      "DDI 12 accuracy: 0.857421875\n",
      "DDI 13 accuracy: 0.10205078125\n",
      "DDI 14 accuracy: 0.90966796875\n",
      "DDI 15 accuracy: 0.6650390625\n",
      "DDI 16 accuracy: 0.98681640625\n",
      "DDI 17 accuracy: 0.81640625\n",
      "DDI 18 accuracy: 0.8837890625\n",
      "DDI 19 accuracy: 0.916015625\n",
      "DDI 20 accuracy: 0.9970703125\n",
      "DDI 21 accuracy: 0.89697265625\n",
      "DDI 22 accuracy: 0.8955078125\n",
      "DDI 23 accuracy: 0.90234375\n",
      "DDI 24 accuracy: 0.92431640625\n",
      "DDI 25 accuracy: 0.97265625\n",
      "DDI 26 accuracy: 0.3447265625\n",
      "DDI 27 accuracy: 0.89013671875\n",
      "DDI 28 accuracy: 0.89794921875\n",
      "DDI 29 accuracy: 0.91943359375\n",
      "DDI 30 accuracy: 0.90966796875\n",
      "DDI 31 accuracy: 0.96728515625\n",
      "DDI 32 accuracy: 0.96337890625\n",
      "DDI 33 accuracy: 0.88037109375\n",
      "DDI 34 accuracy: 0.7841796875\n",
      "DDI 35 accuracy: 0.4892578125\n",
      "DDI 36 accuracy: 0.77392578125\n",
      "DDI 37 accuracy: 0.8564453125\n",
      "DDI 38 accuracy: 0.974609375\n",
      "DDI 39 accuracy: 0.9501953125\n",
      "DDI 40 accuracy: 0.78369140625\n",
      "DDI 41 accuracy: 0.82666015625\n",
      "DDI 42 accuracy: 0.8642578125\n",
      "DDI 43 accuracy: 0.95166015625\n",
      "DDI 44 accuracy: 0.97412109375\n",
      "DDI 45 accuracy: 0.93896484375\n",
      "DDI 46 accuracy: 0.35546875\n",
      "DDI 47 accuracy: 0.9287109375\n",
      "DDI 48 accuracy: 0.7880859375\n",
      "DDI 49 accuracy: 0.888671875\n",
      "DDI 50 accuracy: 0.4951171875\n",
      "DDI 51 accuracy: 0.94287109375\n",
      "DDI 52 accuracy: 0.40087890625\n",
      "DDI 53 accuracy: 0.81494140625\n",
      "DDI 54 accuracy: 0.93310546875\n",
      "DDI 55 accuracy: 0.95751953125\n",
      "DDI 56 accuracy: 0.859375\n",
      "DDI 57 accuracy: 0.8232421875\n",
      "DDI 58 accuracy: 0.83544921875\n",
      "DDI 59 accuracy: 0.8544921875\n",
      "DDI 60 accuracy: 0.92333984375\n",
      "DDI 61 accuracy: 0.90625\n",
      "DDI 62 accuracy: 0.7919921875\n",
      "DDI 63 accuracy: 0.935546875\n",
      "DDI 64 accuracy: 0.54296875\n",
      "DDI 65 accuracy: 0.68310546875\n",
      "DDI 66 accuracy: 0.90087890625\n",
      "DDI 67 accuracy: 0.16845703125\n",
      "DDI 68 accuracy: 0.80712890625\n",
      "DDI 69 accuracy: 0.80810546875\n",
      "DDI 70 accuracy: 0.90185546875\n",
      "DDI 71 accuracy: 0.994140625\n",
      "DDI 72 accuracy: 0.66845703125\n",
      "DDI 73 accuracy: 0.96044921875\n",
      "DDI 74 accuracy: 0.81201171875\n",
      "DDI 75 accuracy: 0.86279296875\n",
      "DDI 76 accuracy: 0.07373046875\n",
      "DDI 77 accuracy: 0.974609375\n",
      "DDI 78 accuracy: 0.80859375\n",
      "DDI 79 accuracy: 0.7607421875\n",
      "DDI 80 accuracy: 0.99755859375\n",
      "DDI 81 accuracy: 0.8173828125\n",
      "DDI 82 accuracy: 0.97265625\n",
      "DDI 83 accuracy: 0.96630859375\n",
      "DDI 84 accuracy: 0.9521484375\n",
      "DDI 85 accuracy: 0.9775390625\n",
      "DDI 86 accuracy: 0.79296875\n"
     ]
    }
   ],
   "source": [
    "# Test set evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs_left, inputs_right, labels = data\n",
    "        inputs_left, inputs_right = inputs_left.to(device), inputs_right.to(device)\n",
    "\n",
    "        outputs = model(inputs_left, inputs_right)\n",
    "        predicted_labels = (outputs.squeeze() > 0.47).float()\n",
    "        predictions.extend(predicted_labels.tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "ddi_accuracies = {}\n",
    "for ddi_type in range(len(true_labels[0])):\n",
    "  ddi_true = [true_label[ddi_type] for true_label in true_labels]\n",
    "  ddi_pred = [pred_label[ddi_type] for pred_label in predictions]\n",
    "  ddi_accuracy = accuracy_score(ddi_true, ddi_pred)\n",
    "  ddi_accuracies[ddi_type] = ddi_accuracy\n",
    "  print(f'DDI {ddi_type+1} accuracy: {ddi_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caG2nW6x2Ifs",
    "outputId": "9a142bd8-21c6-4d53-c162-87765edd8bd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7843556958575582"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(ddi_accuracies.values()))/len(ddi_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GE_Gi6AGm2M_"
   },
   "source": [
    "**Results**\n",
    "\n",
    "Below is the summarized table of results for both the original hypothesis of whether we can accurately predict hold-out DDI samples (90%+ reported in the paper), and for the ablations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Suw3lf4rOH7z"
   },
   "source": [
    "| Number of Layers | Hidden Dimension Size | PCA Dimension | Average DDI Accuracy |\n",
    "|------------------|-----------------------|---------------|---------------|\n",
    "| 2                | 128                   | NA            | 0.818         |\n",
    "| 2                | 128                   | 10            | 0.818         |\n",
    "| 2                | 128                   | 30            | 0.784         |\n",
    "| 2                | 128                   | 50            | 0.789         |\n",
    "| 4                | 128                   | NA            | 0.784         |\n",
    "| 4                | 128                   | 50            | 0.743         |\n",
    "| 8                | 128                   | NA            | 0.754         |\n",
    "| 8                | 128                   | 50            | 0.560         |\n",
    "| 2                | 2048                  | NA            | 0.981         |\n",
    "| 2                | 2048                  | 50            | 0.961         |\n",
    "| 4                | 2048                  | NA            | 0.984         |\n",
    "| 4                | 2048                  | 50            | 0.975         |\n",
    "| 8                | 2048                  | NA            | 0.948         |\n",
    "| 8                | 2048                  | 50            | 0.961         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6caaKjunG-_"
   },
   "source": [
    "We can see from the results that several models performed well above 90% in average DDI accuracy. The paper reported that the 9-layer model with a hidden dimension size of 2,048 performed the best, however we saw that the 4-layer model with a hidden dimension size of 2,048 performed best. We can also see that all models with a hidden dimension size of 2,048 achieved 90%+ mean accuracy. Therefore, it is likely that even with our small subset of the full dataset that we can accurately predict unseen DDIs.\n",
    "\n",
    "For the ablations, we first modified the complexity of the model by scaling the number of layers and hidden dimension size. We can see the trend that with the paper's 50 principal component dimensionality reduction and holding hidden dimension size constant, there is a negative correlation between the number of layers and performance. It is possible that with our reduced dataset that the model is overfitting, and the shallower models have better generalization.\n",
    "\n",
    "Additionally, there is a clear positive correlation between model performance and the hidden dimension size, however intermediate hidden dimension sizes between 128 and 2048 were not tested.\n",
    "\n",
    "Finally, we can see with our ablation study determining the impact of the PCA dimensionality reduction of the molecular fingerprint, there is no clear trend. Holding the number of layers (2) and the hidden dimension size (128) constant, we can there may be a negative correlation between the PCA dimension size and performance, however this contradicts the original paper's findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YELSFLJEpZZ1"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "The paper was fairly difficult to reproduce without the refrence to ChemicalX [2]. The amount of compute given the dataset size also made it difficult to reproduce at the same scale. The original paper [1] does include a link to their repository containing \"source code,\" however it does not contain any code for training the models nor does it contain any model weights. Moreover, the dataset was fairly intensive for preprocessing, and the paper does not provide a clean version of their data for immediate use. That being said, the model itself is very simple being a standard feed-forward network, so model implementation was straightforward.\n",
    "\n",
    "Overall, the findings and takeaways from the study still hold true in that predicting unseen drug-drug interactions is possible using molecular fingerprints based on structural similarity.\n",
    "\n",
    "I would recommend to the authors that the model definition and trianing code be included in the repository, and the data used with SMILES strings be released as well as opposed to only listing the Drug Bank IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cP0tNBwO7iny"
   },
   "source": [
    "**References**\n",
    "\n",
    "1. Ryu, J. Y., Kim, H. U., & Lee, S. Y. (2018). Deep learning improves prediction of drugâ€“drug and drugâ€“food interactions. Proceedings of the national academy of sciences, 115(18), E4304-E4311.\n",
    "\n",
    "\n",
    "2. Rozemberczki, B., Hoyt, C. T., Gogleva, A., Grabowski, P., Karis, K., Lamov, A., ... & Gyori, B. M. (2022, August). Chemicalx: A deep learning library for drug pair scoring. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 3819-3828)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
